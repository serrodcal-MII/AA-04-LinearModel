{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modelos Lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los ejemplos y la discusión que sigue está tomada del libro:\n",
    "\n",
    "[*Introduction to Machine Learning with Python*](http://shop.oreilly.com/product/0636920030515.do)  \n",
    "**Andreas C. Müller & Sarah Guido**  \n",
    "O'Reilly 2017\n",
    "\n",
    "Github con el material del libro: [Github](https://github.com/amueller/introduction_to_ml_with_python). \n",
    "\n",
    "El libro está accesible *online* desde la [Biblioteca de la Universidad de Sevilla](https://fama.us.es), como recurso electrónico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATENCIÓN**: Antes que nada, cargamos el módulo `mglearn`, que se puede descargar del  [Github](https://github.com/amueller/introduction_to_ml_with_python) del libro anterior. Recordar que para que funcione la carga, debemos poner la carpeta `mglearn` en cualquiera de las carpetas que usa python para cargar sus módulos (normalmente, funcionará colocando la carpeta `mglearn` en la misma carpeta en la que se coloque este notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTE 1: Regresión lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos en primer lugar el conjunto de datos `wave`, que viene definido en el módulo `mglearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.ylim(-3, 3)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función (implementada en el módulo `mglearn`) muestra gráficamente el resultado de hacer regresión lineal sobre ese conjunto de datos `wave`. Esta es no es una función de scikit_learn, tan solo es para que veamos gráficamente una recta de mínimos cuadrados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_regression_wave()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos ahora las herramientas de scikit_learn y lo aplicamos al conjunto de datos `wave` (en realidad el que aparece en el dibujo de ariba es una versión reducida con 40 ejemplos, ahora lo aplicamos a 60 ejemplos). Nótese cómo usamos el método de scikit_learn `LinearRegression` (que lleva a cabo el método de mínimos cuadrados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X, y = mglearn.datasets.make_wave(n_samples=60)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el rendimiento de regresión lineal sobre el conjunto de entrenamiento y el de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos incluso consultar los coeficientes (los pesos) de la recta que se ha encontrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendimientos del 0.67 y 0.61 sobre entrenamiento y prueba no son muy buenos. En este caso claramente no hay sobreajuste porque el rendimiento sobre entrenamiento es bastante bajo. Lo que ocurre es que el modelo lineal es probablemente demasiado simple para que se ajuste mejor que eso a ese conjunto de datos unidimensional. En dimensiones superiores, los modelos lineales son más potentes, y sí que corren riesgo de sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro conjunto muy usado para ilustrar regresión es el del precio de la vivienda en Boston, en el que ya tenemos más atributos. En él se predice el precio de una vivienda en función de una serie de 13 atributos, como por ejemplo el número de habitaciones o determinados indicadores del barrio. Más detalles [aquí](https://scikit-learn.org/stable/datasets/index.html#boston-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(\"Filas y columnas de los datos: {}\".format(boston.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos ahora regresión lineal sobre un conjunto mucho mayor (el del **precio de la vivienda en Boston**) en el que se han extendido a 104 las características (sólo 13 de ellas son originales, el resto se han derivado de esas 13 para conseguir artificialmente un conjunto de datos con un número alto de atributos). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí que se produce un claro sobreajuste: alto rendimiento sobre elconjunto de entrenamiento,pero bastante bajo sobre el de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regresión L2 (o  _ridge_)\n",
    "\n",
    "Recordemos que en regresión _ridge_ se introduce un término de _regularización_: la suma de los cuadrados de los coeficientes ($\\sum_i|w_i|^2$)  penaliza el modelo, dependiendo también de un coeficiente que indica el peso de dicha penalización. En scikit-learn, en el caso de regresión ese coeficiente es _alpha_. Cuanto mayor el coeficiente, más regularización. Por defecto es 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, al hacer regularización baja el rendimiento sobre el conjunto de entrenamiento, pero sube sobre el de test (ya que en principio estamos generalizando el modelo aprendido). Veamos qué pasa al variar el parámetro de regularización: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente gráfico nos muestra cómo cambian los distintos coeficientes $w_i$ dependiendo de la regularización que se use. Como se ve, a mayor regularización, los coeficientes son menores en magnitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
    "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
    "\n",
    "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "plt.xlabel(\"Índice del coeficiente\")\n",
    "plt.ylabel(\"Magnitud del coeficiente\")\n",
    "xlims = plt.xlim()\n",
    "plt.hlines(0, xlims[0], xlims[1])\n",
    "plt.xlim(xlims)\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de entender la regularización es fijar _alpha_ pero variar el tamaño del conjunto de entrenamiento. En la siguiente gráfica, se va aumentando el tamaño del conjunto de entrenamiento, se aplica regresión lineal con y sin regularización, y vamos viendo cómo se comporta el modelo aprendido, tanto en el conjunto de entrenamiento, como en el de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_ridge_n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "\n",
    "- Sobre el conjunto de entrenamiento, el rendimiento es mayor que sobre test\n",
    "- Con regularización, el rendimiento es menor sobre entrenamiento, pero mayor sobre test\n",
    "- Con menos datos y sin regularización, se aprende poco. A medida que aumentan los datos, la regularización influye menos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso\n",
    "\n",
    "En regresión _lasso_ (o con regularización $L_1$) el termino de regularización es la suma de los valores absolutos de los coeficientes ($\\sum_i|w_i|$), penalización que también depende de un coeficiente _alpha_ que indica el peso de dicha penalización. Una de las consecuencias más interesantes de usar esta penalización es que algunos de los coeficientes $w_i$ se anulan. En los siguientes ejemplos, además de imprimir el rendimiento, vamos a imprimir también cuántos coeficientes son cero.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Número de coeficientes (pesos) no nulos: {}\".format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el rendimiento es muy malo: demasiada regularización y se usan sólo cuatro características. Habrá que bajar el coeficiente alpha, que por defecto es 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se aumenta max_iter respecto del valor por defecto, \n",
    "# ya que si no da un warning de convergencia:\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
    "print(\"Número de coeficientes (pesos) no nulos: {}\".format(np.sum(lasso001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
    "print(\"Número de coeficientes (pesos) no nulos: {}\".format(np.sum(lasso00001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que con alpha igual a 0.01 la cosa va bastante bien, incluso algo mejor que con _ridge_, ya demás sólo se usan 33 de las 104. Sin embargo, bajar demasiado la regularización ha sido peor, parece ser que se sobreajusta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer también el experimento de ver cómo afecta la regularización lasso a la magnitud de los coeficientes aprendidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\n",
    "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
    "plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
    "\n",
    "plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Índice del coeficiente\")\n",
    "plt.ylabel(\"Magnitud del coeficiente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que para `alpha=1`, la mayoría de los coeficientes son cero, y los que no son cero son pequeños. Con `alpha=0.01`, también hay muchos coeficientes que salen igual a cero. Al bajar la regularización a `alpha=0.0001`, ya practicamente todos los coeficientes son distintos de cero y en el modelo práctucamente no hay regularización. \n",
    "\n",
    "Los puntos rojos corresponden con regularización *ridge* para `alpha=0.1`, que hemos visto en rendimiento que es similar a *lasso* con `alpha=0.01`, sin embargo no se anulan tantos coeficientes. Aunque en general se prefiere *ridge* a *lasso*, si el objetivo es seleccionar características, porque se espera que algunas de ellas no sean relevantes, entonces es preferible usar regularización *lasso*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Clasificación con regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejamos ya los problemas de regresión y pasamos a ver problemas de clasificación. En concreto en esta sección usaremos  regresión logística con el conjunto de datos del cáncer de mama, que ya hemos visto en otemas anteriores. Recordemos que se trata de una base de datos con 30 características y 569 datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el conjunto en entrenamiento y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos ahora la clase `LogisticRegression`, que implementa en scikit-learn regresión logística con regularización incorporada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atención**: En los modelos de clasificación que veremos en este módulo (regresión logística y SVM) la regularización se controla con una constante `C` que supone **el inverso de la cantidad de regularización**. Es decir: cuanto menor el valor de `C`, mayor regularización. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el comportamiento de regresión logística con estos datos, usando la regularización que viene por defecto ($L_2$ o *ridge* con $C=1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"Rendimiento sobre entrenamiento: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de prueba: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rendimiento es bueno. Pero probablemente estamos \"infraajustándonos\", ya que el rendimiento en entrenamiento y prueba son similares. Bajamos la regularización (es decir, aumentamos el valor de `C`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "print(\"Rendimiento sobre entrenamiento: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de prueba: {:.3f}\".format(logreg100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que se confirma que el modelo que salía con la regularización por defecto era demasiado simple. Ahora hemos obtenido un rendimiento mejor sobre el conjunto de prueba, permitiendo una mayor complejidad del modelo (menos regularización)\n",
    "\n",
    "¿Qué pasa si, por el contrario, hacemos más regularización? Pues que los resultados empeoran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\n",
    "print(\"Rendimiento sobre entrenamiento: {:.3f}\".format(logreg001.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de prueba: {:.3f}\".format(logreg001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hicimos con regresión, podemos mostrar con una gráfica cómo afecta la regularización a los coeficientes que se aprenden para cada característica, pero ahora en el caso de los datos sobre el cáncer de mama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "xlims = plt.xlim()\n",
    "plt.hlines(0, xlims[0], xlims[1])\n",
    "plt.xlim(xlims)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Cracterística\")\n",
    "plt.ylabel(\"Magnitud del coeficiente\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora el efecto de la regularización $L_1$ en regresión logística. Como se muestra en las imágenes, se anulan muchos de los coeficientes (más cuanto mayor es la regularización):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
    "    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n",
    "    print(\"Rendimiento en conjunto de entrenamiento, regularización L1, C={:.3f}: {:.2f}\".format(\n",
    "          C, lr_l1.score(X_train, y_train)))\n",
    "    print(\"Rendimiento en conjunto de prueba, regularización L1,  C={:.3f}: {:.2f}\".format(\n",
    "          C, lr_l1.score(X_test, y_test)))\n",
    "    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n",
    "\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "xlims = plt.xlim()\n",
    "plt.hlines(0, xlims[0], xlims[1])\n",
    "plt.xlim(xlims)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "\n",
    "plt.ylim(-5, 5)\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Clasificación múltiple con modelos lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera natural, por su propia definición, los clasificadores lineales son binarios. En scikit-learn, por defecto, la extensión a multiclase de estos clasificadores lineales se hace con el esquema \"uno frente al resto\" (*one vs rest*, también llamado *one vs all*) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ilustremos esto con el conjunto de datos siguiente, generado aleatoriamente por la función de scikit-learn llamada `make_blobs`. Es bidimensonal (dos características) y tiene tres posibles clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Característica 0\")\n",
    "plt.ylabel(\"Característica 1\")\n",
    "plt.legend([\"Clase 0\", \"Clase 1\", \"Clase 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando `LogisticRegression` a este problema multiclase, se aplica *one vs rest*. Esto quiere decir que se aprenden tres clasificadores binarios: tres rectas que marcan la frontera de decisión de cada clase frente al resto, cada una con sus pesos correspondientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_m = LogisticRegression().fit(X, y)\n",
    "print(\"Dimensión de la matriz de coeficientes: \", logreg_m.coef_.shape)\n",
    "print(\"Dimensión de los intercept: \", logreg_m.intercept_.shape)\n",
    "print(logreg_m.coef_)\n",
    "print(logreg_m.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(logreg_m.coef_, logreg_m.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"Característica 0\")\n",
    "plt.ylabel(\"Característica 1\")\n",
    "plt.legend(['Clase 0', 'Clase 1', 'Clase 2', 'Recta clase 0', 'Recta clase 1',\n",
    "            'Recta clase 2'], loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente gráfico muestra, con colores, qué predicción se haría en cada punto del plano con el clasificador aprendido. Nótese que en la zona central (el triángulo que se forma con las tres rectas), los puntos no se clasifican positivamente para ninguna de las regiones. En esos casos, el esquema *one vs rest* hace que la predicción sea la de la recta más cercana al punto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_2d_classification(logreg_m, X, fill=True, alpha=.7)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(logreg_m.coef_, logreg_m.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.legend(['Clase 0', 'Clase 1', 'Clase 2', 'Recta clase 0', 'Recta clase 1',\n",
    "            'Recta clase 2'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"Cacaterística 0\")\n",
    "plt.ylabel(\"Característica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4 Máquinas de vectores soporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que sigue trata de aplicar SVM lineal (sin kernel) a un conjunto de datos que se denomina *forge*, en el que hay **dos clases**. En scikit-learn, se implementa las máquinas de vectores soporte sin Kernel se aplican con la clase `LinearSVC`. \n",
    "\n",
    "Se incluyen a continuación gráfica con las *fronteras de decisión* que se ha aprendido al usar SVM, y se compara con la que se obtienes usando regresión logística: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "                                    ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(\"{}\".format(clf.__class__.__name__))\n",
    "    ax.set_xlabel(\"Característica 0\")\n",
    "    ax.set_ylabel(\"Característica 1\")\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ambos casos, se aplica regularización $L_2$ por defecto, con coeficiente de regularización `C=1`. Recuérdese que este coeficiente representa el inverso de la cantidad de regularización. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que sigue es la gráfica de las fronteras de decisión que se aprenden con `LinearSVC`, con distintos valores de `C`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_svc_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, cuanto mayor es el valor de `C`, menos regularización y más trata el clasificador de ajustarse a **todos** los ejemplos. Cuanto menor es el valor, menos importante es clasificar bien todos los ejemplos, y más el encontrar un buen margen de separción. \n",
    "\n",
    "En la derecha tenemos mucha regularización: hay dos puntos mal clasificados, pero la línea es bastante horizontal. En la figura central bajamos la regularización, y eso hace que el modelo intente algo más el no tener errores en la clasificación. Finalmente, en la izquierda tenemos muy poca regularización: se consigue clasificar correctamente todos los círculos (y solo deja uno de los triángulos mal clasificado), pero no parece que la frontera de decisión capture bien la disposición de los puntos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un ejemplo de frontera de decisión no lineal\n",
    "\n",
    "De nuevo con la función `make_blobs`, generamos aleatoriamente un conjunto de datos bidimensionales, con dos clases, en el que los datos se disponen alrededor de cuatro \"centros\", dos por cada clase:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=4, random_state=8)\n",
    "y = y % 2\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Característica 0\")\n",
    "plt.ylabel(\"Característica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está claro que un modelo puramente lineal se comportará difícilmente puede modelar este problema de clasificación, como podemos ver a continuación. Usemos `LinearSVC` y dibujemos la frontera de decisión que define el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(linear_svm, X)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Característica 0\")\n",
    "plt.ylabel(\"Característica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, veamos gráficamente qué ocurre si añadimos una tercera característica, obtenida simplemente como el cuadrado de la segunda característica del problema original. Para la representación gráfica usamos `Axes3D` de `matplotlib`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add the squared first feature\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "figure = plt.figure()\n",
    "# visualize in 3D\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "# plot first all the points with y==0, then all with y == 1\n",
    "mask = y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.set_xlabel(\"caract0\")\n",
    "ax.set_ylabel(\"caract1\")\n",
    "ax.set_zlabel(\"caract1 ** 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este espacio tridimensional, el conjunto de datos aumentado sí que es linealmente separable. De hecho, podemos aplicar `LinearSVC`al conjunto de datos aumentado, y representar gráficamente el plano de separación que encuantra tras entrenar el modelo con estos datos aumentados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "# show linear decision boundary\n",
    "figure = plt.figure()\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "\n",
    "ax.set_xlabel(\"caract0\")\n",
    "ax.set_ylabel(\"caract1\")\n",
    "ax.set_zlabel(\"caract1 ** 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ahora representar en dos dimensiones (es decir, solo respecto de las características originales) las regiones de decisión que el clasificador ha encontrado. Como se observa, ya no están definidos mediante una línea recta, sino por algo elipsoidal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = YY ** 2\n",
    "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "             cmap=mglearn.cm2, alpha=0.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Característica 0\")\n",
    "plt.ylabel(\"Característica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Máquinas de vectores soporte con kernel\n",
    "\n",
    "Como se ha visto, se abren nuevas perspectivas para los modeos líneales cuando se añaden más características, sumergiendo los datos originales es espacios de una mayor dimensión. Esto podría suponer un problema de eficiencia, pero el llamado \"kernel trick\" nos permite buscar un clasificador lineal (basado en máquines de vectores soporte) en en un espacio de mayores dimensiones, pero **sin usar explícitamente la nueva representación extendida** (ver detalles en las diapositivas).\n",
    "\n",
    "En scikit-learn, las máquinas de vectores soporte con kernels están implementadas por la clase `SVC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora un conjunto de datos bidimensionales con clasificación binaria, usando la función de `mglearn` llamada `make_handcrafted_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = mglearn.tools.make_handcrafted_dataset()\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Característica 0\")\n",
    "plt.ylabel(\"Característica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos ahora usando `SVC`, con kernel gaussiano, anchura de kernel 0.1 (parámetro `gamma`) y regularización 10 (parámetro `C`). En la siguiente gráfica se ve la frontera de decisión encontrada, junto con los vectores soporte (remarcados en la gráfica). Los vectores soporte son aquellos ejemplos del conjunto de entrenamiento que marcan la frontera de decisión. Los *coeficientes duales* marcan la \"importancia\" de cada vector soporte a la hora de clasifucar nuevos ejemplos.  \n",
    "\n",
    "Para clasificar un nuevo ejemplo, solo se usan los vectores soporte: se mide la \"distancia\" a cada vector soporte, y con esas distancias, teniendo en cuenta la importancia de cada uno de ellos, se realiza la predicción. En la gráfica se muestran también las dos regiones de decisión que se han aprendido: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\n",
    "mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "# plot support vectors\n",
    "sv = svm.support_vectors_\n",
    "#print(sv)\n",
    "# class labels of support vectors are given by the sign of the dual coefficients\n",
    "#print(svm.dual_coef_.ravel())\n",
    "sv_labels = svm.dual_coef_.ravel() > 0\n",
    "\n",
    "mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
    "plt.xlabel(\"Característica 0\")\n",
    "plt.ylabel(\"Característica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajuste de (hiper)-parámetros en máquinas de vectores soporte\n",
    "\n",
    "En general, las máquinas de vectores soporte son bastante sensibles a los posibles valores de los parámetros que se usen. Por ejemplo, veamos qué pasa cuando variamos los valores de los parámetros `C` y `gamma`. Para entender esto mejor, damos una intuición de lo que significa cada uno de ellos:\n",
    "\n",
    "- Como en `LinearSVC`, el parámetro `C` indica la intensidad de la regularización. Cuanto mayor, menos regularización, y por tanto modelos más complejos, más ajustados al conjunto de entrenamiento. Cuanto menor, modelo más simples, menos sobreajuste pero también riesgo de ser demasiados simples y tener infraajuste. \n",
    "\n",
    "- El parámetro `gamma` es un parámetro específico del kernel de base radial: $K(v,w)=e^{(-\\gamma\\cdot||v-w||^2)}$. Es el inverso de la anchura del nucleo gaussiano,  y mide la importancia que tiene la distancia a cada ejemplo del conjunto de entrenamiento, a la hora de clasificar una nueva instancia. Cuanto más bajo el valor, más influencia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente figura se observan distintos clasificadores aprendidos con distintas combinaciones de `C` y de `gamma`. De izquierda a derecha, se varía de mayor a menor radio del núcleo: mayor radio significa que muchos puntos se consideran \"cerca\", y esto hace que las fronteras de decisión sean más \"suaves\". De arriba a abajo se pasa de mayor a menor regularización, y por tanto va aumentenda la complejidad de la forntera aprendido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, C in zip(axes, [-1, 0, 3]):\n",
    "    for a, gamma in zip(ax, range(-1, 2)):\n",
    "        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
    "        \n",
    "axes[0, 0].legend([\"clase 0\", \"clase 1\", \"v.s. clase 0\", \"v.s. clase 1\"],\n",
    "                  ncol=4, loc=(.9, 1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicando `SVC` a los datos de cáncer de mama\n",
    "\n",
    "Veamos qué pasa al aplicar maquinas de vectores soporte con kernel (con los parámetros por defecto) al conjunto de datos del cáncer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "19dca39b-9061-4fc6-9aab-f759854ec208"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(svc.score(X_train, y_train)))\n",
    "print(\"Rendimiento sobre el cojunto de prueba: {:.2f}\".format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, un rendimiento bastante malo sobre el conjunto de prueba. Las máquinas de vectores soporte son bastante sensibles a el ajuste de los hiperparámetros, y también **a la escala de los datos** y cada una de sus características. \n",
    "\n",
    "En la siguiente gráfica se observa en qué intervalo se mueven cada una de las características. Hay bastante diferencia entre las magnitides de cada una de ellas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(X_train, manage_xticks=False)\n",
    "plt.yscale(\"symlog\")\n",
    "plt.xlabel(\"Índice de la característica\")\n",
    "plt.ylabel(\"Magnitud de la característica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para intentar paliar esta diferencia de escalado, podemos transformar los datos, haciendo que el rango de valores de cada característica sea el intervalo $[0,1]$. Para ello podemos usar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "\n",
    "print(\"Mínimo de cada característica\\n{}\".format(X_train_scaled.min(axis=0)))\n",
    "print(\"Máximo de cada característica\\n {}\".format(X_train_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos **la misma transformación** al conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos con el conjuto de datos transformado, y comprobamos el rendimiento con el conjunto de prueba igualmente transformado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Rendimiento sobre conjunto de entrenamiento: {:.3f}\".format(\n",
    "        svc.score(X_train_scaled, y_train)))\n",
    "print(\"Rendimiento sobre conjunto de prueba: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bastante mejor. Pero aún se observa cierto infraajuste (sobre el entrenamiento se comporta peor que sobre el conjunto de prueba), así que podríamos ver qué pasa si aminoramos la regularización. Efectivamente se observa una mejora en el rendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1000)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Rendimiento sobre conjunto de entrenamiento: {:.3f}\".format(\n",
    "    svc.score(X_train_scaled, y_train)))\n",
    "print(\"Rendimiento sobre conjunto de prueba: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_metadata": {
   "author": "Andreas C. M\\\"ller",
   "title": "Machine Learning with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
